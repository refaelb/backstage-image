'use strict';

var backendCommon = require('@backstage/backend-common');
var pluginSearchBackendNode = require('@backstage/plugin-search-backend-node');
var uuid = require('uuid');

async function queryPostgresMajorVersion(knex) {
  if (knex.client.config.client !== "pg") {
    throw new Error("Can't resolve version, not a postgres database");
  }
  const { rows } = await knex.raw("SHOW server_version_num");
  const [result] = rows;
  const version = +result.server_version_num;
  const majorVersion = Math.floor(version / 1e4);
  return majorVersion;
}

const migrationsDir = backendCommon.resolvePackagePath(
  "@backstage/plugin-search-backend-module-pg",
  "migrations"
);
class DatabaseDocumentStore {
  constructor(db) {
    this.db = db;
  }
  static async create(database) {
    var _a;
    const knex = await database.getClient();
    try {
      const majorVersion = await queryPostgresMajorVersion(knex);
      if (majorVersion < 12) {
        throw new Error(
          `The PgSearchEngine requires at least postgres version 12 (but is running on ${majorVersion})`
        );
      }
    } catch {
      throw new Error(
        "The PgSearchEngine is only supported when using a postgres database (>=12.x)"
      );
    }
    if (!((_a = database.migrations) == null ? void 0 : _a.skip)) {
      await knex.migrate.latest({
        directory: migrationsDir
      });
    }
    return new DatabaseDocumentStore(knex);
  }
  static async supported(knex) {
    try {
      const majorVersion = await queryPostgresMajorVersion(knex);
      return majorVersion >= 12;
    } catch {
      return false;
    }
  }
  async transaction(fn) {
    return await this.db.transaction(fn);
  }
  async getTransaction() {
    return this.db.transaction();
  }
  async prepareInsert(tx) {
    await tx.raw(
      "CREATE TEMP TABLE documents_to_insert (type text NOT NULL, document jsonb NOT NULL, hash bytea NOT NULL GENERATED ALWAYS AS (sha256(replace(document::text || type, '\\', '\\\\')::bytea)) STORED) ON COMMIT DROP"
    );
  }
  async completeInsert(tx, type) {
    await tx.insert(
      tx("documents_to_insert").select(
        "type",
        "document",
        "hash"
      )
    ).into(tx.raw("documents (type, document, hash)")).onConflict("hash").ignore();
    const rowsToDelete = tx("documents").select("documents.hash").leftJoin("documents_to_insert", {
      "documents.hash": "documents_to_insert.hash"
    }).whereNull("documents_to_insert.hash");
    await tx("documents").where({ type }).whereIn("hash", rowsToDelete).delete();
  }
  async insertDocuments(tx, type, documents) {
    await tx("documents_to_insert").insert(
      documents.map((document) => ({
        type,
        document
      }))
    );
  }
  async query(tx, searchQuery) {
    const { types, pgTerm, fields, offset, limit, options } = searchQuery;
    const query = tx("documents");
    if (pgTerm) {
      query.from(tx.raw("documents, to_tsquery('english', ?) query", pgTerm)).whereRaw("query @@ body");
    } else {
      query.from("documents");
    }
    if (types) {
      query.whereIn("type", types);
    }
    if (fields) {
      Object.keys(fields).forEach((key) => {
        const value = fields[key];
        const valueArray = Array.isArray(value) ? value : [value];
        const fieldValueCompare = valueArray.map((v) => ({ [key]: v })).map((v) => JSON.stringify(v));
        const arrayValueCompare = valueArray.map((v) => ({ [key]: [v] })).map((v) => JSON.stringify(v));
        const valueCompare = [...fieldValueCompare, ...arrayValueCompare];
        query.whereRaw(
          `(${valueCompare.map(() => "document @> ?").join(" OR ")})`,
          valueCompare
        );
      });
    }
    query.select("type", "document");
    if (pgTerm && options.useHighlight) {
      const headlineOptions = `MaxWords=${options.maxWords}, MinWords=${options.minWords}, ShortWord=${options.shortWord}, HighlightAll=${options.highlightAll}, MaxFragments=${options.maxFragments}, FragmentDelimiter=${options.fragmentDelimiter}, StartSel=${options.preTag}, StopSel=${options.postTag}`;
      query.select(tx.raw('ts_rank_cd(body, query) AS "rank"')).select(
        tx.raw(
          `ts_headline('english', document, query, '${headlineOptions}') as "highlight"`
        )
      ).orderBy("rank", "desc");
    } else if (pgTerm && !options.useHighlight) {
      query.select(tx.raw('ts_rank_cd(body, query) AS "rank"')).orderBy("rank", "desc");
    } else {
      query.select(tx.raw("1 as rank"));
    }
    return await query.offset(offset).limit(limit);
  }
}

var __defProp$1 = Object.defineProperty;
var __defNormalProp$1 = (obj, key, value) => key in obj ? __defProp$1(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __publicField$1 = (obj, key, value) => {
  __defNormalProp$1(obj, typeof key !== "symbol" ? key + "" : key, value);
  return value;
};
class PgSearchEngineIndexer extends pluginSearchBackendNode.BatchSearchEngineIndexer {
  constructor(options) {
    super({ batchSize: options.batchSize });
    __publicField$1(this, "logger");
    __publicField$1(this, "store");
    __publicField$1(this, "type");
    __publicField$1(this, "tx");
    __publicField$1(this, "numRecords", 0);
    this.store = options.databaseStore;
    this.type = options.type;
    this.logger = options.logger || backendCommon.getVoidLogger();
  }
  async initialize() {
    this.tx = await this.store.getTransaction();
    try {
      await this.store.prepareInsert(this.tx);
    } catch (e) {
      this.tx.rollback(e);
      throw e;
    }
  }
  async index(documents) {
    this.numRecords += documents.length;
    const refs = [...new Set(documents.map((d) => {
      var _a;
      return (_a = d.authorization) == null ? void 0 : _a.resourceRef;
    }))];
    this.logger.debug(
      `Attempting to index the following entities: ${refs.toString()}`
    );
    try {
      await this.store.insertDocuments(this.tx, this.type, documents);
    } catch (e) {
      this.tx.rollback(e);
      throw e;
    }
  }
  async finalize() {
    if (this.numRecords === 0) {
      this.logger.warn(
        `Index for ${this.type} was not replaced: indexer received 0 documents`
      );
      this.tx.rollback();
      return;
    }
    try {
      await this.store.completeInsert(this.tx, this.type);
      this.tx.commit();
    } catch (e) {
      this.tx.rollback(e);
      throw e;
    }
  }
  /**
   * Custom handler covering the case where an error occurred somewhere else in
   * the indexing pipeline (e.g. a collator or decorator). In such cases, the
   * finalize method is not called, which leaves a dangling transaction and
   * therefore an open connection to PG. This handler ensures we close the
   * transaction and associated connection.
   *
   * todo(@backstage/discoverability-maintainers): Consider introducing a more
   * formal mechanism for handling such errors in BatchSearchEngineIndexer and
   * replacing this method with it. See: #17291
   *
   * @internal
   */
  async _destroy(error, done) {
    if (!error) {
      done();
      return;
    }
    if (!this.tx.isCompleted()) {
      await this.tx.rollback(error);
    }
    done(error);
  }
}

var __defProp = Object.defineProperty;
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __publicField = (obj, key, value) => {
  __defNormalProp(obj, typeof key !== "symbol" ? key + "" : key, value);
  return value;
};
class PgSearchEngine {
  /**
   * @deprecated This will be marked as private in a future release, please us fromConfig instead
   */
  constructor(databaseStore, config, logger) {
    this.databaseStore = databaseStore;
    __publicField(this, "logger");
    __publicField(this, "highlightOptions");
    __publicField(this, "indexerBatchSize");
    var _a, _b, _c, _d, _e, _f, _g, _h;
    const uuidTag = uuid.v4();
    const highlightConfig = config.getOptionalConfig(
      "search.pg.highlightOptions"
    );
    const highlightOptions = {
      preTag: `<${uuidTag}>`,
      postTag: `</${uuidTag}>`,
      useHighlight: (_a = highlightConfig == null ? void 0 : highlightConfig.getOptionalBoolean("useHighlight")) != null ? _a : true,
      maxWords: (_b = highlightConfig == null ? void 0 : highlightConfig.getOptionalNumber("maxWords")) != null ? _b : 35,
      minWords: (_c = highlightConfig == null ? void 0 : highlightConfig.getOptionalNumber("minWords")) != null ? _c : 15,
      shortWord: (_d = highlightConfig == null ? void 0 : highlightConfig.getOptionalNumber("shortWord")) != null ? _d : 3,
      highlightAll: (_e = highlightConfig == null ? void 0 : highlightConfig.getOptionalBoolean("highlightAll")) != null ? _e : false,
      maxFragments: (_f = highlightConfig == null ? void 0 : highlightConfig.getOptionalNumber("maxFragments")) != null ? _f : 0,
      fragmentDelimiter: (_g = highlightConfig == null ? void 0 : highlightConfig.getOptionalString("fragmentDelimiter")) != null ? _g : " ... "
    };
    this.highlightOptions = highlightOptions;
    this.indexerBatchSize = (_h = config.getOptionalNumber("search.pg.indexerBatchSize")) != null ? _h : 1e3;
    this.logger = logger;
  }
  /**
   * @deprecated This will be removed in a future release, please us fromConfig instead
   */
  static async from(options) {
    return new PgSearchEngine(
      await DatabaseDocumentStore.create(options.database),
      options.config,
      options.logger
    );
  }
  static async fromConfig(config, options) {
    return new PgSearchEngine(
      await DatabaseDocumentStore.create(options.database),
      config,
      options.logger
    );
  }
  static async supported(database) {
    return await DatabaseDocumentStore.supported(await database.getClient());
  }
  translator(query, options) {
    const pageSize = query.pageLimit || 25;
    const { page } = decodePageCursor(query.pageCursor);
    const offset = page * pageSize;
    const limit = pageSize + 1;
    return {
      pgQuery: {
        pgTerm: query.term.split(/\s/).map((p) => p.replace(/[\0()|&:*!]/g, "").trim()).filter((p) => p !== "").map((p) => `(${JSON.stringify(p)} | ${JSON.stringify(p)}:*)`).join("&"),
        fields: query.filters,
        types: query.types,
        offset,
        limit,
        options: options.highlightOptions
      },
      pageSize
    };
  }
  setTranslator(translator) {
    this.translator = translator;
  }
  async getIndexer(type) {
    var _a;
    return new PgSearchEngineIndexer({
      batchSize: this.indexerBatchSize,
      type,
      databaseStore: this.databaseStore,
      logger: (_a = this.logger) == null ? void 0 : _a.child({ documentType: type })
    });
  }
  async query(query) {
    const { pgQuery, pageSize } = this.translator(query, {
      highlightOptions: this.highlightOptions
    });
    const rows = await this.databaseStore.transaction(
      async (tx) => this.databaseStore.query(tx, pgQuery)
    );
    const { page } = decodePageCursor(query.pageCursor);
    const hasNextPage = rows.length > pageSize;
    const hasPreviousPage = page > 0;
    const pageRows = rows.slice(0, pageSize);
    const nextPageCursor = hasNextPage ? encodePageCursor({ page: page + 1 }) : void 0;
    const previousPageCursor = hasPreviousPage ? encodePageCursor({ page: page - 1 }) : void 0;
    const results = pageRows.map(
      ({ type, document, highlight }, index) => ({
        type,
        document,
        rank: page * pageSize + index + 1,
        highlight: {
          preTag: pgQuery.options.preTag,
          postTag: pgQuery.options.postTag,
          fields: highlight ? {
            text: highlight.text,
            title: highlight.title,
            location: highlight.location,
            path: ""
          } : {}
        }
      })
    );
    return { results, nextPageCursor, previousPageCursor };
  }
}
function decodePageCursor(pageCursor) {
  if (!pageCursor) {
    return { page: 0 };
  }
  return {
    page: Number(Buffer.from(pageCursor, "base64").toString("utf-8"))
  };
}
function encodePageCursor({ page }) {
  return Buffer.from(`${page}`, "utf-8").toString("base64");
}

exports.DatabaseDocumentStore = DatabaseDocumentStore;
exports.PgSearchEngine = PgSearchEngine;
//# sourceMappingURL=PgSearchEngine-7d93bdd7.cjs.js.map
